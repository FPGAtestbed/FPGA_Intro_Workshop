# Dataflow and Stream

The `dataflow` pragma can be used to implement task level parallelism. The Vitis tools will analyse the dataflow through the region of code in which the `dataflow` pragma appears, and attempts to analyse the data dependencies in order to determine if functions can be performed in pipelined. Data which is generated by a producer loop / function is immediately forwarded to the consumer loop / function, rather than waiting for the entire input to be produced before starting the consumer. 

To get the most out of `dataflow` regions you should make use of `streams` to move data from one loop / function to the next; this helps to enforce the single-producer-consumer model necessary for the `dataflow` analysis. 

## Syntax

### Dataflow Syntax:
`#pragma HLS dataflow`

The `dataflow` pragma doesn't take any options! 
Find out more about the conditions of `dataflow` and how to use it in the docs: https://docs.xilinx.com/r/en-US/ug1399-vitis-hls/pragma-HLS-dataflow

### Stream Syntax:

`#pragma HLS stream type=<type> depth=<int>`

We'll only use `type` of `fifo`; the `depth` will depend on how much data you need to hold at any given time - a perfectly pipelined loop with II=1 should only need a depth of 1. You can ignore the depth option for now. 

If you prefer you can add the streams header

`#include "hls_stream.h"`

and then declare a stream like so:

`hls::stream<double> myStream;`

You then read / write to the stream using the following commands:

```
myStream.write(2.5);
double x = myStream.read();
```
This enforces the first-in-first-out behaviour of streams, as random access is not allowed! 

## Exercise

See the example in `dataflow_example.cpp`. 
The kernel has a simple data dependency:
```
extern "C"
{
    void dataflow_kernel(double *A, double *B, double *out)
    {
#pragma HLS interface m_axi port=A bundle=memblock_1
#pragma HLS interface m_axi port=B bundle=memblock_2
#pragma HLS interface m_axi port=out bundle=memblock_1

        // We don't want to alter global memory in place! So we have local
        // arrays to hold the result of processing the input arrays 
       double B_prime[N_VALS], A_prime[N_VALS];

        // We apply some procesing function to the two input arrays, which can 
        // be done in parallel
        process_A(A, A_prime);
        process_B(B, B_prime);

        // Add the resulting arrays together. 
        v_add(B_prime, A_prime, out);
    }
}
```
The kernel:
- Takes two input arrays, `A` and `B`
- Processes each array independently using two functions 
- Adds the two resulting arrays together to give the output array

The `v_add` function is dependent on the output of `process_A` and `process_B`. 

1. Run C-Synthesis on the example and note the latency of each of the three loops, and the total latency. Does the design do all three loops sequentially, or do some of them overlap?
2. To get the best performance, we want to run `v_add` on each pair of values as soon as they become available, rather than waiting for entire arrays to finish. This will also allow us to save memory on the board as we don't need to store these entire arrays if we process elements straight away! 
    - Start by converting the intermediate arrays `A_prime` and `B_prime` into streams using a `pragma` or `hls::stream` *. (Optionally, you can also change `A`, `B`, and `out` to streams.) 
    - Re-run C-Synthesis - the latency should still be just as bad (or worse!) because the tool is not analysing the flow of data through these functions yet. 
    - Add a `dataflow` pragma inside the top level function (`dataflow_kernel`), and re-run the synthesis. The latency for the entire kernel should now be similar to a single loop (plus ~70 cycles for read/write requests if using global memory for `A`, `B`, and `out`), because every element of data can be immediately passed forward to the next step in the data processing, essentially forming one large pipelined process.
    - Open the schedule viewer. At the top level (`dataflow_kernel`), you should be able to see that the functios `process_A` and `process_B` are called at the same time, followed by `v_add`. You can also see this by opening the `dataflow viewer` (you can do this by right clicking on `dataflow_kernel` in the synthesis summary). Thanks to the `dataflow` pragma, although `v_add` has to wait for `process_A` and `process_B` to produce an element of data to use, it doesn't have to wait for the entire function to finish processing all of the input vectors. 


\* if using `hls::stream<double>`, you'll need to include `hls_stream.h` and read and write to streams using `.read()` and `.write(value)`. This is generally the recommended way of implementing streams as it enforces stream semantics. 
